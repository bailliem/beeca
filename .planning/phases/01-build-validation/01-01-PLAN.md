---
phase: 01-build-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: []
autonomous: true

must_haves:
  truths:
    - "R CMD check output is captured and visible"
    - "testthat results are captured and visible"
    - "Test coverage statistics are captured and visible"
    - "Any errors, warnings, or failures are clearly identified"
  artifacts:
    - path: ".planning/phases/01-build-validation/01-CHECK-RESULTS.md"
      provides: "R CMD check output, testthat results, and coverage report"
  key_links: []
---

<objective>
Run R CMD check, testthat suite, and coverage analysis to validate the beeca package builds cleanly.

Purpose: Establish baseline validation status before attempting fixes. All checks run independently and results are captured for triage.
Output: CHECK-RESULTS.md documenting R CMD check output, testthat results, and coverage analysis.
</objective>

<execution_context>
@/Users/bailliem/.claude/get-shit-done/workflows/execute-plan.md
@/Users/bailliem/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-build-validation/01-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run R CMD check</name>
  <files>.planning/phases/01-build-validation/01-CHECK-RESULTS.md</files>
  <action>
Run R CMD check on the beeca package using devtools::check().

1. From the package root directory, run:
   ```r
   Rscript -e "devtools::check(error_on = 'never')"
   ```

2. Capture the full output including:
   - Duration
   - Number of ERRORs
   - Number of WARNINGs
   - Number of NOTEs
   - Details of any issues found

3. Create `.planning/phases/01-build-validation/01-CHECK-RESULTS.md` with a section for R CMD check results.

Note: Use `error_on = 'never'` to capture all issues without stopping on first error. We want complete picture for triage.
  </action>
  <verify>
File 01-CHECK-RESULTS.md exists with R CMD check section showing error/warning/note counts.
  </verify>
  <done>
R CMD check has been run and results are documented with counts of errors, warnings, and notes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run testthat suite</name>
  <files>.planning/phases/01-build-validation/01-CHECK-RESULTS.md</files>
  <action>
Run the testthat suite for the beeca package.

1. From the package root directory, run:
   ```r
   Rscript -e "devtools::test()"
   ```

2. Capture:
   - Total tests run
   - Tests passed
   - Tests failed
   - Tests skipped
   - Any failure details

3. Append testthat results to `.planning/phases/01-build-validation/01-CHECK-RESULTS.md`.

Note: devtools::test() runs all tests in tests/testthat/. Per context, skip slow cross-validation tests if they exist (check for skip conditions in test files).
  </action>
  <verify>
01-CHECK-RESULTS.md contains testthat section with pass/fail/skip counts.
  </verify>
  <done>
testthat suite has been run and results are documented with pass/fail/skip counts.
  </done>
</task>

<task type="auto">
  <name>Task 3: Run test coverage analysis</name>
  <files>.planning/phases/01-build-validation/01-CHECK-RESULTS.md</files>
  <action>
Run test coverage analysis using covr to identify any coverage gaps.

1. From the package root directory, run:
   ```r
   Rscript -e "cov <- covr::package_coverage(); print(cov); covr::report(cov, file = '.planning/phases/01-build-validation/coverage-report.html', browse = FALSE)"
   ```

   If covr is not installed, first run:
   ```r
   Rscript -e "install.packages('covr', repos = 'https://cloud.r-project.org')"
   ```

2. Capture from output:
   - Overall coverage percentage
   - Per-file coverage percentages
   - Functions/lines with 0% coverage (gaps)

3. Append coverage analysis to `.planning/phases/01-build-validation/01-CHECK-RESULTS.md` with:
   - Overall coverage percentage
   - List of files below 80% coverage (for awareness)
   - List of any functions with 0% coverage

Note: Per phase context, there is no specific coverage percentage target. This is for documentation and awareness only. Document any gaps found but they do not block the release.
  </action>
  <verify>
01-CHECK-RESULTS.md contains coverage section with overall percentage and list of any gaps identified.
  </verify>
  <done>
Test coverage analysis has been run and coverage gaps documented for awareness.
  </done>
</task>

<task type="auto">
  <name>Task 4: Summarize validation status</name>
  <files>.planning/phases/01-build-validation/01-CHECK-RESULTS.md</files>
  <action>
Add a summary section to 01-CHECK-RESULTS.md:

1. Create "Validation Summary" section at top of file with:
   - Overall status: PASS (0 errors, 0 warnings, all tests pass) or ISSUES FOUND
   - Quick counts table
   - List of issues requiring triage (if any)

2. If issues found, categorize them:
   - **Blockers**: Errors or test failures that must be fixed
   - **Warnings**: Need evaluation - some may be acceptable
   - **Notes**: Document for awareness, typically don't block release
   - **Skipped tests**: Note why skipped
   - **Coverage gaps**: Document for awareness (no target required)

3. Format the file so it's easy for user to review and make triage decisions.
  </action>
  <verify>
01-CHECK-RESULTS.md has summary section at top with clear status and categorized issues.
  </verify>
  <done>
Validation results are summarized with clear categorization of any issues found.
  </done>
</task>

</tasks>

<verification>
- [ ] R CMD check was run and completed
- [ ] testthat suite was run and completed
- [ ] covr::package_coverage() was run and results captured
- [ ] 01-CHECK-RESULTS.md exists with all results
- [ ] Summary clearly indicates PASS or lists issues for triage
</verification>

<success_criteria>
- R CMD check output captured with error/warning/note counts
- testthat results captured with pass/fail/skip counts
- Coverage analysis captured with overall percentage and gap list
- Results documented in 01-CHECK-RESULTS.md
- Clear summary indicating whether issues need triage in Plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/01-build-validation/01-01-SUMMARY.md`
</output>
